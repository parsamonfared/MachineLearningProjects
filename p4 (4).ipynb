{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "p4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFf1FpR-ZyJe",
        "colab_type": "text"
      },
      "source": [
        "https://colab.research.google.com/drive/1AsNbgRTxjfss3EM18OWLGRIKoQFS-FAh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHk6HSf5yN2e",
        "colab_type": "text"
      },
      "source": [
        "# CSC321 Project 4.\n",
        "\n",
        "**Deadline**: Thursday, April. 2, by 9pm\n",
        "\n",
        "**Submission**: \n",
        "If you are using Google Colab or Jupyter Notebook,\n",
        "the easiest way to submit the assignment is to submit a PDF export of the completed notebook. \n",
        "If you are using Python, please submit a PDF file containing your code, written solutions,\n",
        "and any outputs that we are using to evaluate your work.\n",
        "\n",
        "**Late Submission**: Please see the syllabus for the late submission criteria.\n",
        "\n",
        "This time, we're back to the application of deep learning to natural language processing.\n",
        "We will be working with a subset of Reuters news headlines that are collected over 15 months,\n",
        "covering all of 2019, plus a few months in 2018 and in a few months of this year.\n",
        "\n",
        "In particular, we will be building an **autoencoder** of news headlines. The idea is similar\n",
        "to the kind of image autoencoder we built in lecture: we will have an **encoder** that\n",
        "maps a news headline to a vector embedding, and then a **decoder** that reconstructs\n",
        "the news headline. Both our encoder and decoder networks will be Recurrent Neural Networks,\n",
        "so that you have a chance to practice building\n",
        "\n",
        "- a neural network that takes a sequence as an input\n",
        "- a neural network that generates a sequence as an output\n",
        "\n",
        "This project is organized as follows:\n",
        "\n",
        "- Question 1. Exploring the data\n",
        "- Question 2. Building the autoencoder\n",
        "- Question 3. Training the autoencoder using *data augmentation*\n",
        "- Question 4. Analyzing the embeddings (interpolating between headlines)\n",
        "\n",
        "Furthermore, we'll be introducing the idea of **data augmentation** for improving of\n",
        "the robustness of the autoencoder, as proposed by Shen et al [1].\n",
        "\n",
        "[1] Shen et al (2019) \"Educating Text Autoencoders: Latent Representation Guidance via Denoising\" https://arxiv.org/pdf/1905.12777.pdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuYU-wRRyN2f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKcfZFSTyN2i",
        "colab_type": "text"
      },
      "source": [
        "## Question 1\n",
        "\n",
        "Download the files `reuters_train.txt` and `reuters_valid.txt`, and upload them to Google Drive.\n",
        "\n",
        "Then, mount Google Drive from your Google Colab notebook:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhENb-9fyN2i",
        "colab_type": "code",
        "outputId": "dae471b7-ed8d-4ff7-8a0b-ab01d7ef7114",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "train_path = '/content/gdrive/My Drive/CSC321/reuters_train.txt' # Update me\n",
        "valid_path = '/content/gdrive/My Drive/CSC321/reuters_valid.txt' # Update me"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mgQhgOqyN2l",
        "colab_type": "text"
      },
      "source": [
        "As promised in previous lecture, we will be using PyTorch's `torchtext` utilities to help us load, process,\n",
        "and batch the data. We'll be using a `TabularDataset` to load our data, which works well on structured\n",
        "CSV data with fixed columns (e.g. a column for the sequence, a column for the label). Our tabular dataset\n",
        "is even simpler: we have no labels, just some text. So, we are treating our data as a table with one field\n",
        "representing our sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubExfd_HyN2l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchtext\n",
        "\n",
        "# Tokenization function to separate a headline into words\n",
        "def tokenize_headline(headline):\n",
        "    \"\"\"Returns the sequence of words in the string headline. We also\n",
        "    prepend the \"<bos>\" or beginning-of-string token, and append the\n",
        "    \"<eos>\" or end-of-string token to the headline.\n",
        "    \"\"\"\n",
        "    return (\"<bos> \" + headline + \" <eos>\").split()\n",
        "\n",
        "# Data field (column) representing our *text*.\n",
        "text_field = torchtext.data.Field(\n",
        "    sequential=True,            # this field consists of a sequence\n",
        "    tokenize=tokenize_headline, # how to split sequences into words\n",
        "    include_lengths=True,       # to track the length of sequences, for batching\n",
        "    batch_first=True,           # similar to batch_first=True in nn.RNN demonstrated in lecture\n",
        "    use_vocab=True)             # to turn each character into an integer index\n",
        "train_data = torchtext.data.TabularDataset(\n",
        "    path=train_path,                # data file path\n",
        "    format=\"tsv\",                   # fields are separated by a tab\n",
        "    fields=[('title', text_field)]) # list of fields (we have only one)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmniOaEnyN2o",
        "colab_type": "text"
      },
      "source": [
        "### Part (a) -- 2 points\n",
        "\n",
        "Draw histograms of the number of words per headline in our training set.\n",
        "Excluding the `<bos>` and `<eos>` tags in your computation.\n",
        "Explain why we would be interested in such histograms."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMob8ESmyN2p",
        "colab_type": "code",
        "outputId": "6bfeb35d-95fb-4b8c-82b0-f2976ee61756",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        }
      },
      "source": [
        "# Include your histogram and your written explanations\n",
        "\n",
        "# Here is an example of how to plot a histogram in matplotlib:\n",
        "# plt.hist(np.random.normal(0, 1, 40), bins=20)\n",
        "\n",
        "# Here are some sample code that uses the train_data object:\n",
        "\n",
        "L = []\n",
        "for example in train_data:\n",
        "    L.append(len(example.title)-2)\n",
        "plt.hist(L, bins=20)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([1.1200e+02, 2.9700e+02, 2.9310e+03, 6.1630e+03, 1.2529e+04,\n",
              "        4.8726e+04, 2.9969e+04, 2.6251e+04, 3.1818e+04, 6.8520e+03,\n",
              "        3.3640e+03, 2.0980e+03, 2.2700e+02, 7.7000e+01, 2.0000e+01,\n",
              "        3.0000e+00, 4.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00]),\n",
              " array([ 2.  ,  3.35,  4.7 ,  6.05,  7.4 ,  8.75, 10.1 , 11.45, 12.8 ,\n",
              "        14.15, 15.5 , 16.85, 18.2 , 19.55, 20.9 , 22.25, 23.6 , 24.95,\n",
              "        26.3 , 27.65, 29.  ]),\n",
              " <a list of 20 Patch objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 132
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAD4CAYAAAAD6PrjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZgUVZoEyN2r",
        "colab_type": "text"
      },
      "source": [
        "### Part (b) -- 2 points\n",
        "\n",
        "How many distinct words appear in the training data?\n",
        "Exclude the `<bos>` and `<eos>` tags in your computation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H128EoMgyN2s",
        "colab_type": "code",
        "outputId": "8254bb20-7dd7-40d9-9a33-2cd73f7bc05f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Report your values here. Make sure that you report the actual values,\n",
        "# and not just the code used to get those values\n",
        "\n",
        "# You might find the python class Counter from the collections package useful\n",
        "# import collections\n",
        "# vocab = {}\n",
        "# flat_list = []\n",
        "# for headline in train_data:\n",
        "#   for word in headline.title:\n",
        "#     flat_list.append(word)\n",
        "#     if word in vocab:\n",
        "#       vocab[word] += 1\n",
        "#     else:\n",
        "#       vocab[word] = 1\n",
        "import collections\n",
        "flat_list = []\n",
        "for headline in train_data:\n",
        "  for word in headline.title:\n",
        "    if word not in('<bos>', '<eos>'):\n",
        "      flat_list.append(word)\n",
        "cnt = collections.Counter(flat_list)\n",
        "\n",
        "print(\"there are {} distinct words in the training data\".format(len(cnt)))\n",
        "\n",
        "\n",
        "# there are 51298 distinct words in the training data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "there are 51298 distinct words in the training data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rB7VdEx3yN2u",
        "colab_type": "text"
      },
      "source": [
        "### Part (c) -- 2 points\n",
        "\n",
        "The distribution of *words* will have a long tail, meaning that there are some words\n",
        "that will appear very often, and many words that will appear infrequently. How many words\n",
        "appear exactly once in the training set? Exactly twice?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahjRftECyN2v",
        "colab_type": "code",
        "outputId": "ec4eda78-f788-4133-a873-29032d77dbfb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Report your values here. Make sure that you report the actual values,\n",
        "# and not just the code used to get those values\n",
        "\n",
        "def ones_twos():\n",
        "  ones = 0\n",
        "  twos = 0\n",
        "  for word in cnt.keys():\n",
        "    if cnt[word] == 1:\n",
        "      ones += 1\n",
        "    elif cnt[word] == 2:\n",
        "      twos += 1\n",
        "  return ones, twos\n",
        "\n",
        "ones, twos = ones_twos()\n",
        "print(\"there are {} many number of words that appear excatly once\".format(ones))\n",
        "print(\"there are {} many number of words that appear excatly twice\".format(twos))\n",
        "\n",
        "# there are 19854 many number of words that appear excatly once\n",
        "# there are 7193 many number of words that appear excatly twice"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "there are 19854 many number of words that appear excatly once\n",
            "there are 7193 many number of words that appear excatly twice\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNlbOrv4yN2x",
        "colab_type": "text"
      },
      "source": [
        "### Part (d) -- 2 points\n",
        "\n",
        "Explain why we may wish to replace these infrequent\n",
        "words with an `<unk>` tag, instead of learning embeddings for these rare words.\n",
        "(Hint: Consider words in the validation set that might not appear in training)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-MVsGpqyN2y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# infrequent words do not help us with making predictions.\n",
        "# In fact training our model based on infrequent words will throw\n",
        "# out model off from what it might generaly need to predict/produce"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PF5Kfkm1yN20",
        "colab_type": "text"
      },
      "source": [
        "### Part (e) -- 2 points\n",
        "\n",
        "We will only model the top 9995 words in the training set, excluding the tags\n",
        "`<bos>`, `<eos>`, and other possible tags we haven't mentioned yet\n",
        "(including those, we will have a vocabulary size of exactly 10000 tokens).\n",
        "\n",
        "What percentage of word occurrences will be supported? Alternatively, what percentage\n",
        "of word occurrences in the training set will be set to the `<unk>` tag?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frkwD9adyN21",
        "colab_type": "code",
        "outputId": "b97e009a-93df-4e1f-e510-9df46f6eca91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Report your values here. Make sure that you report the actual values,\n",
        "# and not just the code used to get those values\n",
        "top_words = cnt.most_common(9995)\n",
        "\n",
        "top_words_occur = 0\n",
        "for word in top_words:\n",
        "  top_words_occur += word[1]\n",
        "\n",
        "\n",
        "count = 0\n",
        "for word in cnt.keys():\n",
        "  if cnt[word] == 1 or cnt[word] == 2:\n",
        "    count += cnt[word]\n",
        "  \n",
        "print(\"percentage of occurrences of the top 9995 words: \", (top_words_occur/len(flat_list)) * 100)\n",
        "print(\"percentage of word occurrences in the training set will be set to the <unk> tag: \", count/len(flat_list) * 100)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "percentage of occurrences of the top 9995 words:  93.97857393100142\n",
            "percentage of word occurrences in the training set will be set to the <unk> tag:  1.8038569031506908\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fu2pWvbryN23",
        "colab_type": "text"
      },
      "source": [
        "Our `torchtext` package will help us keep track of our list of unique words, known\n",
        "as a **vocabulary**. A vocabulary also assigns a unique integer index to each word.\n",
        "You can interpret these indices as sparse representations of one-hot vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbH1sh1MyN23",
        "colab_type": "code",
        "outputId": "134f801c-4dbe-447b-a51c-c55f47329f94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "# Build the vocabulary based on the training data. The vocabulary\n",
        "# can have at most 9997 words (9995 words + the <bos> and <eos> token)\n",
        "text_field.build_vocab(train_data, max_size=9997)\n",
        "\n",
        "# This vocabulary object will be helpful for us\n",
        "vocab = text_field.vocab\n",
        "print(vocab.stoi[\"hello\"]) # for instances, we can convert from string to (unique) index\n",
        "print(vocab.itos[10])      # ... and from word index to string\n",
        "\n",
        "# The size of our vocabulary is actually 10000\n",
        "vocab_size = len(text_field.vocab.stoi)\n",
        "print(vocab_size) # should be 10000\n",
        "\n",
        "# The reason is that torchtext adds two more tokens for us:\n",
        "print(vocab.itos[0]) # <unk> represents an unknown word not in our vocabulary\n",
        "print(vocab.itos[1]) # <pad> will be used to pad short sequences for batching"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "on\n",
            "10000\n",
            "<unk>\n",
            "<pad>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHn9wl3nyN26",
        "colab_type": "text"
      },
      "source": [
        "## Question 2\n",
        "\n",
        "Building a text autoencoder is a little more complicated than an image autoencoder, so\n",
        "we'll need to thoroughly understand the model that we want to build before actually building\n",
        "our model. Note that the best and fastest way to complete this assignment is to spend a *lot*\n",
        "of time upfront understanding the architecture. The explanations are quite dense, and you\n",
        "might need to stop every sentence or two to understand what's going on.\n",
        "You won't feel productive for a while since you won't be writing code,\n",
        "but this initial investment will help you become more productive later on.\n",
        "Understanding this architecture will also help you understand other machine learning\n",
        "papers you might come across. So, take a deep breath, and let's do this!\n",
        "\n",
        "Here is a diagram showing our desired architecture:\n",
        "\n",
        "<img src=\"https://www.cs.toronto.edu/~lczhang/321/hw/p4model.png\" width=\"95%\" />\n",
        "\n",
        "There are two main components to the model: the **encoder** and the **decoder**.\n",
        "As always with neural networks, we'll first describe how to make\n",
        "**predictions** with of these components. Let's get started:\n",
        "\n",
        "The **encoder** will take a sequence of words (a headline) as *input*, and produce an\n",
        "embedding (a vector) that represents the entire headline. In the diagram above,\n",
        "the vector ${\\bf h}^{(7)}$ is the vector embedding containing information about \n",
        "the entire headline.  This portion is very similar\n",
        "to the sentiment analysis RNN that we discussed in lecture (but without the fully-connected\n",
        "layer that makes a prediction).\n",
        "\n",
        "The **decoder** will take an embedding (in the diagram, the vector ${\\bf h}^{(7)}$) as input,\n",
        "and uses a separate RNN to **generate a sequence of words**. To generate a sequence of words,\n",
        "the decoder needs to do the following:\n",
        "\n",
        "1) Determine the previous word that was generated. This previous word will act as ${\\bf x}^{(t)}$\n",
        "   to our RNN, and will be used to update the hidden state ${\\bf m}^{(t)}$. Since each of our\n",
        "   sequences begin with the `<bos>` token, we'll set ${\\bf x}^{(1)}$ to be the `<bos>` token.\n",
        "2) Compute the updates to the hidden state ${\\bf m}^{(t)}$ based on the previous hidden state\n",
        "   ${\\bf m}^{(t-1)}$ and ${\\bf x}^{(t)}$. Intuitively, this hidden state vector ${\\bf m}^{(t)}$\n",
        "   is a representation of *all the words we still need to generate*.\n",
        "3) We'll use a fully-connected layer to take a hidden state ${\\bf m}^{(t)}$, and determine\n",
        "   *what the next word should be*. This fully-connected layer solves a *classification problem*,\n",
        "   since we are trying to choose a word out of $K=10000$ distinct words. As in a classification\n",
        "   problem, the fully-connected neural network will compute a *probability distribution* over\n",
        "   these 10,000 words. In the diagram, we are using ${\\bf z}^{(t)}$ to represent the logits,\n",
        "   or the pre-softmax activation values representing the probability distribution.\n",
        "4) We will need to *sample* an actual word from this probability distribution ${\\bf z}^{(t)}$.\n",
        "   We can do this in a number of ways, which we'll discuss in question 3. For now, you can \n",
        "   imagine your favourite way of picking a word given a distribution over words.\n",
        "5) This word we choose will become the next input ${\\bf x}^{(t+1)}$ to our RNN, which is used\n",
        "   to update our hidden state ${\\bf m}^{(t+1)}$---i.e. to determine what are the remaining\n",
        "   words to be generated.\n",
        "\n",
        "We can repeat this process until we see an `<eos>` token generated, or until the generated\n",
        "sequence becomes too long.\n",
        "\n",
        "Unfortunately, we can't *train* this autoencoder in the way we just described. That is,\n",
        "we can't just compare our generated sequence with our ground-truth sequence, and get\n",
        "gradients. Both sequences are **discrete** entities, so we won't be able to compute\n",
        "gradients at all! In particular, **sampling is a discrete process**, and so we won't be\n",
        "able to back-propagate through any kind of sampling that we do.\n",
        "\n",
        "You might wonder whether we can get away with computing gradients by comparing the\n",
        "distributions ${\\bf z}^{(t)}$ with the ground truth words at each time step. Like any\n",
        "multi-class classification problem, we can represent the ground-truth words as a one-hot\n",
        "vector, and use the cross-entropy loss.\n",
        "\n",
        "In theory, we can do this. In practice, there are a few issues. One is that the generated\n",
        "sequence might be longer or shorter than the actual sequence, meaning that there may\n",
        "be more/fewer ${\\bf z}^{(t)}$s than ground-truth words. Another more insidious issue\n",
        "is that the **gradients will become very high-variance and unstable**, because\n",
        "**early mistakes will easily throw the model off-track**. Early in training,\n",
        "our model is unlikely to produce the right answer in step $t=1$, so the gradients\n",
        "we obtain based on the other time steps will not be very useful.\n",
        "\n",
        "At this point, you might have some ideas about \"hacks\" we can use to make training\n",
        "work. Fortunately, there is one very well-established solution called\n",
        "**teacher forcing** which we can use for training:\n",
        "instead of *sampling* the next word based on ${\\bf z}^{(t)}$, we will forgo sampling,\n",
        "and use the **ground truth** ${\\bf x}^{(t)}$ in the next step.\n",
        "\n",
        "Here is a diagram showing how we can use **teacher forcing** to train our model:\n",
        "\n",
        "<img src=\"https://www.cs.toronto.edu/~lczhang/321/hw/p4model_tf.png\" width=\"95%\" />\n",
        "\n",
        "We will use the RNN generator to compute the logits\n",
        "${\\bf z}^{(1)},{\\bf z}^{(2)},  \\cdots {\\bf z}^{(T)}$. These distributions\n",
        "can be compared to the ground-truth words using the cross-entropy loss.\n",
        "The loss function for this model will be the sum of the losses across each $t$.\n",
        "(This is similar to what we did in a pixel-wise prediction problem.)\n",
        "\n",
        "We'll train the encoder and decoder model simultaneously. There are several components\n",
        "to our model that contain tunable weights:\n",
        "\n",
        "- The word embedding that maps a word to a vector representation.\n",
        "  In theory, we could use GloVe embeddings, or initialize our parameters to\n",
        "  GloVe embeddings. To prevent students who don't have Colab access\n",
        "  from having to download a 1GB file, we won't do that.\n",
        "  The word embedding component is represented with blue arrows in the diagram.\n",
        "- The encoder RNN (which will use Gated Recurrent Units) that computes the\n",
        "  embedding over the entire headline. The encoder RNN \n",
        "  is represented with black arrows in the diagram.\n",
        "- The decoder RNN (which will also use Gated Recurrent Units) that computes\n",
        "  hidden states, which are vectors representing what words are to be generated.\n",
        "  The decoder RNN is represented with gray arrows in the diagram.\n",
        "- The **projection MLP** (a fully-connected layer) that computes\n",
        "  a distribution over the next word to generate, given a decoder RNN hidden\n",
        "  state.\n",
        "\n",
        "## Part (a) -- 10 pts\n",
        "\n",
        "Complete the code for the AutoEncoder class below by:\n",
        "\n",
        "1. Filling in the missing numbers in the `__init__` method using\n",
        "   the parameters `vocab_size`, `emb_size`, and `hidden_size`. (4 points)\n",
        "2. Complete the `forward` method, which uses teacher forcing\n",
        "   and computes the logits $z^{(t)}$ of the reconstruction of\n",
        "   the sequence. (4 points)\n",
        "\n",
        "You should first try to understand the `encode` and `decode` methods,\n",
        "which are written for you. The `encode` method mimics the discriminative\n",
        "RNN we wrote in class for sentiment analysis.  The `decode` method is\n",
        "a bit more challenging. You might want to scroll down to the\n",
        "`sample_sequence` function to see how this function will be called.\n",
        "\n",
        "You can (but don't have to) use the `encode` and `decode` method in\n",
        "your `forward` method. In either case, be very careful of the input\n",
        "that you feed into ether `decode` or to `self.decoder_rnn`.\n",
        "Refer to the teacher-forcing diagram."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-ZoAlLQyN26",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AutoEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_size, hidden_size):\n",
        "        \"\"\"\n",
        "        A text autoencoder. The parameters \n",
        "            - vocab_size: number of unique words/tokens in the vocabulary\n",
        "            - emb_size: size of the word embeddings $x^{(t)}$\n",
        "            - hidden_size: size of the hidden states in both the\n",
        "                           encoder RNN ($h^{(t)}$) and the\n",
        "                           decoder RNN ($m^{(t)}$)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(num_embeddings=vocab_size, # TODO\n",
        "                                  embedding_dim=emb_size)  # TODO\n",
        "        self.encoder_rnn = nn.GRU(input_size=emb_size, #TODO\n",
        "                                  hidden_size=hidden_size, #TODO\n",
        "                                  batch_first=True)\n",
        "        self.decoder_rnn = nn.GRU(input_size=emb_size, #not sure\n",
        "                                  hidden_size=hidden_size, #TODO\n",
        "                                  batch_first=True)\n",
        "        self.proj = nn.Linear(in_features=hidden_size, # not sure\n",
        "                              out_features=vocab_size) # TODO\n",
        "\n",
        "                      \n",
        "\n",
        "    def encode(self, inp):\n",
        "        \"\"\"\n",
        "        Computes the encoder output given a sequence of words.\n",
        "        \"\"\"\n",
        "        emb = self.embed(inp)\n",
        "        out, last_hidden = self.encoder_rnn(emb)\n",
        "        return last_hidden\n",
        "\n",
        "    def decode(self, inp, hidden=None):\n",
        "        \"\"\"\n",
        "        Computes the decoder output given a sequence of words, and\n",
        "        (optionally) an initial hidden state.\n",
        "        \"\"\"\n",
        "        emb = self.embed(inp)\n",
        "        out, last_hidden = self.decoder_rnn(emb, hidden)\n",
        "        out_seq = self.proj(out)\n",
        "        return out_seq, last_hidden\n",
        "\n",
        "    def forward(self, inp):\n",
        "        \"\"\"\n",
        "        Compute both the encoder and decoder forward pass\n",
        "        given an integer input sequence inp with shape [batch_size, seq_length],\n",
        "        with inp[a,b] representing the (index in our vocabulary of) the b-th word\n",
        "        of the a-th training example.\n",
        "\n",
        "        This function should return the logits $z^{(t)}$ in a tensor of shape\n",
        "        [batch_size, seq_length - 1, vocab_size], computed using *teaching forcing*.\n",
        "\n",
        "        The (seq_length - 1) part is not a typo. If you don't understand why\n",
        "        we need to subtract 1, refer to the teacher-forcing diagram above.\n",
        "        \"\"\"\n",
        "        batch_size, seq_length = inp.shape\n",
        "        hidden = self.encode(inp)\n",
        "        zs, last_hidden = self.decode(inp, hidden) \n",
        "        return zs\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDjoXSUWyN29",
        "colab_type": "text"
      },
      "source": [
        "### Part (b) -- 5 pts\n",
        "\n",
        "To check that your model is set up correctly, we'll train our AutoEncoder\n",
        "neural network for at least 300 iterations to memorize this sequence:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7Z1r9GXyN2-",
        "colab_type": "code",
        "outputId": "7a0a5071-cf93-41b2-ce82-36ed58a093b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\n",
        "headline = train_data[42].title\n",
        "print(headline)\n",
        "input_seq = torch.Tensor([vocab.stoi[w] for w in headline]).long().unsqueeze(0)\n",
        "print(input_seq)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['<bos>', 'zambian', 'president', 'swears', 'in', 'new', 'army', 'chief', '<eos>']\n",
            "tensor([[   2, 5258,   91, 9117,    6,   25,  637,  118,    3]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nVGMkLqyN3A",
        "colab_type": "text"
      },
      "source": [
        "We are looking for the way that you set up your loss function\n",
        "corresponding to the figure above.\n",
        "**Be very careful of off-by-ones.**\n",
        "\n",
        "\n",
        "Note that the Cross Entropy Loss expects a rank-2 tensor as its first\n",
        "argument, and a rank-1 tensor as its second argument. You will\n",
        "need to properly reshape your data to be able to compute the loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZe9MXAtyN3A",
        "colab_type": "code",
        "outputId": "e97b4210-a1e2-4288-decf-ffc0759c38f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "model = AutoEncoder(vocab_size, 128, 128)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for it in range(300):\n",
        "    #TODO\n",
        "    optimizer.zero_grad()\n",
        "    output= model(input_seq[:,:-1]) # with no EOS\n",
        "\n",
        "    loss = criterion(output.reshape(-1, vocab_size), # reshape to 2D tensor\n",
        "                     input_seq[:,1:].reshape(-1))    # reshape to 1D tensor with no BOS\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (it+1) % 50 == 0:\n",
        "        print(\"[Iter %d] Loss %f\" % (it+1, float(loss)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Iter 50] Loss 0.103038\n",
            "[Iter 100] Loss 0.025797\n",
            "[Iter 150] Loss 0.016103\n",
            "[Iter 200] Loss 0.011169\n",
            "[Iter 250] Loss 0.008235\n",
            "[Iter 300] Loss 0.006399\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mZ73gW8yN3C",
        "colab_type": "text"
      },
      "source": [
        "### Part (c) -- 2 pt\n",
        "\n",
        "Once you are satisfied with your model, encode your input using\n",
        "the RNN encoder, and sample some sequences from the decoder. The \n",
        "sampling code is provided to you, and performs the computation\n",
        "from the first diagram (without teacher forcing).\n",
        "\n",
        "Note that we are sampling from a multi-nomial distribution described\n",
        "by the logits $z^{(t)}$. For example, if our distribution is [80%, 20%]\n",
        "over a vocabulary of two words, then we will choose the first word\n",
        "with 80% probability and the second word with 20% probability.\n",
        "\n",
        "Call `sample_sequence` at least 5 times, with the default temperature\n",
        "value. Make sure to include the generated sequences in your PDF\n",
        "report."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bP9_ZrQlyN3D",
        "colab_type": "code",
        "outputId": "9d2d98fa-8198-4edb-eaeb-c5ed32eed0f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "def sample_sequence(model, hidden, max_len=20, temperature=1):\n",
        "    \"\"\"\n",
        "    Return a sequence generated from the model's decoder\n",
        "        - model: an instance of the AutoEncoder model\n",
        "        - hidden: a hidden state (e.g. computed by the encoder)\n",
        "        - max_len: the maximum length of the generated sequence\n",
        "        - temperature: described in Part (d)\n",
        "    \"\"\"\n",
        "    # We'll store our generated sequence here\n",
        "    generated_sequence = []\n",
        "    # Set input to the <BOS> token\n",
        "    inp = torch.Tensor([text_field.vocab.stoi[\"<bos>\"]]).long()\n",
        "    for p in range(max_len):\n",
        "        # compute the output and next hidden unit\n",
        "        output, hidden = model.decode(inp.unsqueeze(0), hidden)\n",
        "        # Sample from the network as a multinomial distribution\n",
        "        output_dist = output.data.view(-1).div(temperature).exp()\n",
        "        top_i = int(torch.multinomial(output_dist, 1)[0])\n",
        "        # Add predicted word to string and use as next input\n",
        "        word = text_field.vocab.itos[top_i]\n",
        "        # Break early if we reach <eos>\n",
        "        if word == \"<eos>\":\n",
        "            break\n",
        "        generated_sequence.append(word)\n",
        "        inp = torch.Tensor([top_i]).long()\n",
        "    return generated_sequence\n",
        "\n",
        "# Your solutions go here\n",
        "for it in range(5):\n",
        "  hidden = model.encode(input_seq)\n",
        "  print(sample_sequence(model, hidden))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['zambian', 'president', 'swears', 'in', 'new', 'army', 'chief']\n",
            "['zambian', 'president', 'swears', 'in', 'new', 'army', 'chief']\n",
            "['zambian', 'president', 'swears', 'in', 'new', 'army', 'chief']\n",
            "['zambian', 'president', 'swears', 'in', 'new', 'army', 'chief']\n",
            "['zambian', 'president', 'swears', 'in', 'new', 'army', 'chief']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gA0GbFpNyN3F",
        "colab_type": "text"
      },
      "source": [
        "### Part (d) -- 3 pt\n",
        "\n",
        "The multi-nomial distribution can be manipulated using the `temperature`\n",
        "setting. This setting can be used to make the distribution \"flatter\" (e.g.\n",
        "more likely to generate different words) or \"peakier\" (e.g. less likely\n",
        "to generate different words).\n",
        "\n",
        "Call `sample_sequence` at least 5 times each for at least 3 different\n",
        "temperature settings (e.g. 1.5, 2, and 5). Explain why we generally\n",
        "don't want the temperature setting to be too **large**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bi1o_YJ_yN3F",
        "colab_type": "code",
        "outputId": "e5c01c62-f7f1-4a9c-9d08-ccb26c96a653",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 619
        }
      },
      "source": [
        "# Include the generated sequences and explanation in your PDF report.\n",
        "print(\"sequences generated with temperature of 1:\\n\")\n",
        "for it in range(5):\n",
        "  hidden = model.encode(input_seq)\n",
        "  print(sample_sequence(model, hidden, temperature=1))\n",
        "\n",
        "print(\"sequences generated with temperature of 1.5:\\n\")\n",
        "for it in range(5):\n",
        "  hidden = model.encode(input_seq)\n",
        "  print(sample_sequence(model, hidden, temperature=1.5))\n",
        "print(\"\\n\\nsequences generated with temperature of 2:\\n\")\n",
        "for it in range(5):\n",
        "  hidden = model.encode(input_seq)\n",
        "  print(sample_sequence(model, hidden, temperature=2))\n",
        "print(\"\\n\\nsequences generated with temperature of 5:\\n\")\n",
        "for it in range(5):\n",
        "  hidden = model.encode(input_seq)\n",
        "  print(sample_sequence(model, hidden, temperature=5))\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sequences generated with temperature of 1:\n",
            "\n",
            "['zambian', 'president', 'swears', 'in', 'new', 'army', 'chief']\n",
            "['zambian', 'president', 'swears', 'in', 'new', 'army', 'chief']\n",
            "['zambian', 'president', 'swears', 'in', 'new', 'army', 'chief']\n",
            "['zambian', 'president', 'swears', 'in', 'new', 'army', 'chief']\n",
            "['zambian', 'president', 'swears', 'in', 'new', 'army', 'chief']\n",
            "sequences generated with temperature of 1.5:\n",
            "\n",
            "['beef', 'president', 'swears', 'in', 'new', 'army', 'chief']\n",
            "['alexa', 'born', 'kings', 'student', 'president', 'reduce', 'cartels', 'unprecedented', 'swears', 'in', 'new', 'army', 'chief', 'assess', '_num_-google', 'midland', 'cnpc', 'latam', 'concessions', 'agents']\n",
            "['e-cigs', 'president', 'swears', 'constitution', 'tentative', 'doubles', 'army', 'trying', 'confident', 'embattled', 'finalise', 'center-right', 'canola', 'colonial', 'swears', 'monitors', 'new', 'army', 'chief']\n",
            "['zambian', 'president', 'swears', 'liability', 'president', 'swears', 'in', 'caesars', 'upheaval', 'joe', 'in', 'new', 'fintechs', 'schedule', 'wings', 'chief']\n",
            "['zambian', 'president', 'covered', 'relief', 'schedule', 'u-turn', 'boosted', 'in', 'new', 'army', 'chief']\n",
            "\n",
            "\n",
            "sequences generated with temperature of 2:\n",
            "\n",
            "['fy', 'xl', 'berrettini', 'least', 'discounts', 'dollar', 'badly', 'deploy', 'challenges', 'pocket', 'pop', 'disorder', 'sunday', 'count', 'army', 'etf', 'wisconsin', 'sinks', 'dive', 'holding']\n",
            "['privatization', 'bottle', 'roads', 'kurdish', 'gray', 'conducts', 'fame', 'amro', 'widespread', 'religious', 'casts', 'libra', 'irna', \"shi'ite\", 'lula', 'starmer', 'bird', 'gun', 'slowing', 'carter']\n",
            "['decided', 'blank', 'storage', '?', 'wpp', '_num_-cn', 'pressed', 'customs', 'reforms', 'new', 'crimes', 'traders', 'challenges', 'mlb', ';', 'ags', 'ramping', 'convertible', 'complaints', 'villa']\n",
            "['excluded', 'corp', 'nothing', 'lot', 'hanoi', 'new', 'species', 'automakers', 'grounding', 'ally', 'inevitable', 'dams', 'wood', 'draghi', 'sparkle', 'out', 'signs', 'khartoum', 'formally', 'hiding']\n",
            "['immigrants', 'australian', 'itf', 'legislators', 'higher', 'champ', 'mps', 'bullet', 'president', 'bt', 'participation', 'let', 'stormy', 'curry', 'similar', 'detail', 'enclave', 'flow', 'lessons', 'islamist']\n",
            "\n",
            "\n",
            "sequences generated with temperature of 5:\n",
            "\n",
            "['suspects', 'outperform', 'serving', 'ball', 'bhp', 'inside', 'helped', 'colombian', 'failures', 'coca-cola', 'kipchoge', 'wh', 'why', 'rugby', 'msci', 'awaited', 'wta', 'gundlach', 'escort', 'trans']\n",
            "['gross', 'senegal', 'disaster', 'williams', 'edges', 'buffalo', 'fao', 'exodus', 'retiring', 'left', 'chef', 'rivian', 'suv', 'unaware', 'jgbs', '_num_-japan', 'paying', '_num_-anadarko', 'article', 'lithuania']\n",
            "['_num_-santander', 'sierra', 'minnesota', 'places', 'fighters', 'deposit', 'uniper', 'nerve', 'three-day', 'operations', 'guzman', 'crisis-hit', '-statement', 'arkansas', 'emir', 'days', 'calling', 'cold', 'architect', 'tighter']\n",
            "['di', 'breaks', 'bezos', 'heartland', 'ca', 'wood', 'darfur', 'bishop', 'triggered', 'sentiment', 'scooters', 'mess', 'teachers', 'tread', 'pray', 'armenian', 'snubs', 'sciences', 'networks', 'opioids']\n",
            "['medicine', 'drives', 'route', 'isolated', 'jr.', 'tepco', 'epa', '_num_-kkr', 'alexa', 'rift', 'birds', 'hardline', 'countries', 'centres', 'radio', 'hollywood', 'aviva', 'wealth', 'plummet', 'once']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FR0dbDKmaHq5",
        "colab_type": "text"
      },
      "source": [
        "**explaination**: if the temperature setting is too high we going to have a less precise response and a higher probobility of having words in our generated sequence that don't make sense.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y82RdquQyN3H",
        "colab_type": "text"
      },
      "source": [
        "## Question 3\n",
        "\n",
        "It turns out that getting good results from a text auto-encoder is very difficult,\n",
        "and that it is very easy for our model to **overfit**. We have discussed several methods\n",
        "that we can use to prevent overfitting, and we'll introduce one more today:\n",
        "**data augmentation**.\n",
        "\n",
        "The idea behind data augmentation is to artificially increase the number of training\n",
        "examples by \"adding noise\" to the image. For example, during AlexNet training,\n",
        "the authors randomly cropped $224\\times 224$\n",
        "regions of a $256 \\times 256$ pixel image to increase the amount of training data.\n",
        "The authors also flipped the image left/right (but not up/down---why?).\n",
        "Machine learning practitioners can also add Gaussian noise to the image.\n",
        "\n",
        "When we use data augmentation to train an *autoencoder*, we typically to only add\n",
        "the noise to the input, and expect the reconstruction to be *noise free*.\n",
        "This makes the task of the autoencoder even more difficult. An autoencoder trained\n",
        "with noisy inputs is called a **denoising auto-encoder**. For simplicity, we will\n",
        "*not* build a denoising autoencoder today.\n",
        "\n",
        "### Part (a) -- 3pt\n",
        "\n",
        "Give three more examples of data augmentation techniques that we could use if\n",
        "we were training an **image** autoencoder. What are different ways that we can\n",
        "change our input?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqrT7mu5yN3I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Techniques:\n",
        "\n",
        "#  1. Rotation: we could rotate our image with a certain small angle and pad the empty space caused by rotation.\n",
        "#  2. Scalling: we could scale our image down and pad around it or scalde up by some factor.\n",
        "#  3. brightness: increasing or decreasing the brightness of all pixels by some factor."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPgZLsh8yN3K",
        "colab_type": "text"
      },
      "source": [
        "### Part (b) -- 2pt\n",
        "\n",
        "We will add noise to our headlines using a few different techniques:\n",
        "\n",
        "1. Shuffle the words in the headline, taking care that words don't end up too far from where they were initially\n",
        "2. Drop (remove) some words \n",
        "3. Replace some words with a blank word (a `<pad>` token)\n",
        "4. Replace some words with a random word \n",
        "\n",
        "The code for adding these types of noise is provided for you:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXCKo_8kyN3L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize_and_randomize(headline,\n",
        "                           drop_prob=0.1,  # probability of dropping a word\n",
        "                           blank_prob=0.1, # probability of \"blanking\" out a word\n",
        "                           sub_prob=0.1,   # probability of substituting a word with a random one\n",
        "                           shuffle_dist=3): # maximum distance to shuffle a word\n",
        "    \"\"\"\n",
        "    Add 'noise' to a headline by slightly shuffling the word order,\n",
        "    dropping some words, blanking out some words (replacing with the <pad> token)\n",
        "    and substituting some words with random ones.\n",
        "    \"\"\"\n",
        "    headline = [vocab.stoi[w] for w in headline.split()]\n",
        "    n = len(headline)\n",
        "    # shuffle\n",
        "    headline = [headline[i] for i in get_shuffle_index(n, shuffle_dist)]\n",
        "\n",
        "    new_headline = [vocab.stoi['<bos>']]\n",
        "    for w in headline:\n",
        "        if random.random() < drop_prob:\n",
        "            # drop the word\n",
        "            pass\n",
        "        elif random.random() < blank_prob:\n",
        "            # replace with blank word\n",
        "            new_headline.append(vocab.stoi[\"<pad>\"])\n",
        "        elif random.random() < sub_prob:\n",
        "            # substitute word with another word\n",
        "            new_headline.append(random.randint(0, vocab_size - 1))\n",
        "        else:\n",
        "            # keep the original word\n",
        "            new_headline.append(w)\n",
        "    new_headline.append(vocab.stoi['<eos>'])\n",
        "    return new_headline\n",
        "\n",
        "def get_shuffle_index(n, max_shuffle_distance):\n",
        "    \"\"\" This is a helper function used to shuffle a headline with n words,\n",
        "    where each word is moved at most max_shuffle_distance. The function does\n",
        "    the following: \n",
        "       1. start with the *unshuffled* index of each word, which\n",
        "          is just the values [0, 1, 2, ..., n]\n",
        "       2. perturb these \"index\" values by a random floating-point value between\n",
        "          [0, max_shuffle_distance]\n",
        "       3. use the sorted position of these values as our new index\n",
        "    \"\"\"\n",
        "    index = np.arange(n)\n",
        "    perturbed_index = index + np.random.rand(n) * 3\n",
        "    new_index = sorted(enumerate(perturbed_index), key=lambda x: x[1])\n",
        "    return [index for (index, pert) in new_index]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tc6Abfo5yN3N",
        "colab_type": "text"
      },
      "source": [
        "Call the function `tokenize_and_randomize` 5 times on a headline of your\n",
        "choice. Make sure to include both your original headline, and the five new\n",
        "headlines in your report."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VA92UyxVyN3N",
        "colab_type": "code",
        "outputId": "16f99ad6-cd75-4fd1-a7b9-2c7dfb56627f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        }
      },
      "source": [
        "headline = train_data[42].title\n",
        "original = [vocab.stoi[w] for w in headline]\n",
        "headline = \" \".join(headline[1:-1])\n",
        "print(headline)\n",
        "print(\"Original Headline: \", original, \"-->\", train_data[42].title)\n",
        "for it in range(5):\n",
        "  print(\"Headline\", it+1, tokenize_and_randomize(headline), \"-->\", [vocab.itos[w] for w in tokenize_and_randomize(headline)])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "zambian president swears in new army chief\n",
            "Original Headline:  [2, 5258, 91, 9117, 6, 25, 637, 118, 3] --> ['<bos>', 'zambian', 'president', 'swears', 'in', 'new', 'army', 'chief', '<eos>']\n",
            "Headline 1 [2, 5258, 9117, 91, 6, 637, 118, 3] --> ['<bos>', 'javid', 'swears', 'president', 'in', 'chief', 'army', '<eos>']\n",
            "Headline 2 [2, 1, 91, 9117, 637, 1, 4081, 3] --> ['<bos>', 'zambian', 'president', 'in', 'swears', 'army', 'new', '<eos>']\n",
            "Headline 3 [2, 5258, 91, 8988, 6, 25, 118, 637, 3] --> ['<bos>', 'president', 'zambian', '<pad>', 'in', 'army', '<pad>', 'chief', '<eos>']\n",
            "Headline 4 [2, 5258, 91, 9117, 6, 25, 637, 118, 3] --> ['<bos>', 'zambian', 'president', '<pad>', 'in', 'new', 'chief', 'army', '<eos>']\n",
            "Headline 5 [2, 91, 5258, 9117, 6, 25, 637, 118, 3] --> ['<bos>', 'president', 'zambian', 'in', 'hsbc', 'new', 'army', 'chief', '<eos>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ixQL8v-yN3P",
        "colab_type": "text"
      },
      "source": [
        "### Part (c) -- 3 pt\n",
        "\n",
        "The training code that we use to train the model is mostly provided for you. \n",
        "The only part we left blank are the parts from Q2(b). Complete the code,\n",
        "and train a new AutoEncoder model for 1 epoch. You can train your model\n",
        "for longer if you want, but training tend to take a long time,\n",
        "so we're only checking to see that your training loss is trending down.\n",
        "\n",
        "If you are using Google Colab, you can use a GPU for this portion.\n",
        "Go to \"Runtime\" => \"Change Runtime Type\"  and set \"Hardware acceleration\" to GPU.\n",
        "Your Colab session will restart.\n",
        "You can move your model to the GPU by typing `model.cuda()`, and move\n",
        "other tensors to GPU (e.g. `xs = xs.cuda()`). To move a model back to CPU,\n",
        "type `model.cpu`. To move a tensor back, use `xs = xs.cpu()`. For training,\n",
        "your model and inputs need to be on the *same device*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CY6XnyLUyN3P",
        "colab_type": "code",
        "outputId": "9e906155-c007-4430-db64-9619929aefe3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def train_autoencoder(model, batch_size=64, learning_rate=0.001, num_epochs=10):\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    model.cuda()\n",
        "    train_loss = []\n",
        "    iterations = []\n",
        "\n",
        "\n",
        "    for ep in range(num_epochs):\n",
        "        # We will perform data augmentation by re-reading the input each time\n",
        "        field = torchtext.data.Field(sequential=True,\n",
        "                                     tokenize=tokenize_and_randomize, # <-- data augmentation\n",
        "                                     include_lengths=True,\n",
        "                                     batch_first=True,\n",
        "                                     use_vocab=False, # <-- the tokenization function replaces this\n",
        "                                     pad_token=vocab.stoi['<pad>'])\n",
        "        dataset = torchtext.data.TabularDataset(train_path, \"tsv\", [('title', field)])\n",
        "\n",
        "        # This BucketIterator will handle padding of sequences that are not of the same length\n",
        "        train_iter = torchtext.data.BucketIterator(dataset,\n",
        "                                                   batch_size=batch_size,\n",
        "                                                   sort_key=lambda x: len(x.title), # to minimize padding\n",
        "                                                   repeat=False)\n",
        "        for it, ((xs, lengths), _) in enumerate(train_iter):\n",
        "          xs = xs.cuda()\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "          output= model(xs[:,:-1]) # with no EOS\n",
        "\n",
        "          loss = criterion(output.reshape(-1, vocab_size), # reshape to 2D tensor\n",
        "                     xs[:,1:].reshape(-1))    # reshape to 1D tensor with no BOS\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          if (it+1) % 100 == 0:\n",
        "            train_loss.append(loss)\n",
        "            iterations.append(ep*2600 + it+1)\n",
        "            \n",
        "\n",
        "            print(\"[Iter %d] Loss %f\" % (ep*2600 + it+1, float(loss)))\n",
        "\n",
        "        #Optional: Compute and track validation loss\n",
        "        # val_loss = 0\n",
        "        # val_n = 0\n",
        "        # for it, ((xs, lengths), _) in enumerate(valid_iter):\n",
        "        #    zs = model(xs)\n",
        "        #    loss =  criterion(output.reshape(-1, vocab_size),\n",
        "        #              xs[:,1:].reshape(-1))\n",
        "        #    val_loss += float(loss)\n",
        "    plt.title(\"Learning Curve: Loss per Iteration\")\n",
        "    plt.plot(iterations, train_loss, label=\"Train\")\n",
        "    plt.xlabel(\"Iterations\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.show()\n",
        "train_autoencoder(AutoEncoder(vocab_size, 128, 128))\n",
        "# Include your training curve or output to show that your training loss is trending down"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Iter 100] Loss 4.403461\n",
            "[Iter 200] Loss 4.418451\n",
            "[Iter 300] Loss 4.181628\n",
            "[Iter 400] Loss 4.512764\n",
            "[Iter 500] Loss 4.180304\n",
            "[Iter 600] Loss 3.411060\n",
            "[Iter 700] Loss 4.190422\n",
            "[Iter 800] Loss 3.343110\n",
            "[Iter 900] Loss 3.463022\n",
            "[Iter 1000] Loss 3.327314\n",
            "[Iter 1100] Loss 3.757345\n",
            "[Iter 1200] Loss 3.622746\n",
            "[Iter 1300] Loss 3.581578\n",
            "[Iter 1400] Loss 3.727996\n",
            "[Iter 1500] Loss 3.428750\n",
            "[Iter 1600] Loss 3.507546\n",
            "[Iter 1700] Loss 3.655648\n",
            "[Iter 1800] Loss 3.387524\n",
            "[Iter 1900] Loss 3.536270\n",
            "[Iter 2000] Loss 3.068127\n",
            "[Iter 2100] Loss 3.114302\n",
            "[Iter 2200] Loss 3.629774\n",
            "[Iter 2300] Loss 3.118087\n",
            "[Iter 2400] Loss 3.078005\n",
            "[Iter 2500] Loss 3.209658\n",
            "[Iter 2600] Loss 3.529156\n",
            "[Iter 2700] Loss 2.861953\n",
            "[Iter 2800] Loss 2.784741\n",
            "[Iter 2900] Loss 3.435439\n",
            "[Iter 3000] Loss 2.281250\n",
            "[Iter 3100] Loss 3.057751\n",
            "[Iter 3200] Loss 3.170620\n",
            "[Iter 3300] Loss 3.466186\n",
            "[Iter 3400] Loss 2.961241\n",
            "[Iter 3500] Loss 3.065810\n",
            "[Iter 3600] Loss 3.432473\n",
            "[Iter 3700] Loss 2.980274\n",
            "[Iter 3800] Loss 2.710616\n",
            "[Iter 3900] Loss 2.767652\n",
            "[Iter 4000] Loss 2.801694\n",
            "[Iter 4100] Loss 2.625039\n",
            "[Iter 4200] Loss 3.065979\n",
            "[Iter 4300] Loss 2.328939\n",
            "[Iter 4400] Loss 2.587312\n",
            "[Iter 4500] Loss 3.029625\n",
            "[Iter 4600] Loss 3.171759\n",
            "[Iter 4700] Loss 2.547570\n",
            "[Iter 4800] Loss 2.734969\n",
            "[Iter 4900] Loss 2.999270\n",
            "[Iter 5000] Loss 2.281104\n",
            "[Iter 5100] Loss 2.492243\n",
            "[Iter 5200] Loss 2.911292\n",
            "[Iter 5300] Loss 2.942237\n",
            "[Iter 5400] Loss 2.569952\n",
            "[Iter 5500] Loss 2.715585\n",
            "[Iter 5600] Loss 2.561529\n",
            "[Iter 5700] Loss 2.452854\n",
            "[Iter 5800] Loss 2.876018\n",
            "[Iter 5900] Loss 2.339806\n",
            "[Iter 6000] Loss 2.457608\n",
            "[Iter 6100] Loss 2.161311\n",
            "[Iter 6200] Loss 2.327984\n",
            "[Iter 6300] Loss 2.350814\n",
            "[Iter 6400] Loss 2.342543\n",
            "[Iter 6500] Loss 2.360624\n",
            "[Iter 6600] Loss 2.302396\n",
            "[Iter 6700] Loss 2.740513\n",
            "[Iter 6800] Loss 2.598812\n",
            "[Iter 6900] Loss 2.380917\n",
            "[Iter 7000] Loss 2.334434\n",
            "[Iter 7100] Loss 2.417063\n",
            "[Iter 7200] Loss 2.287709\n",
            "[Iter 7300] Loss 1.991196\n",
            "[Iter 7400] Loss 2.348284\n",
            "[Iter 7500] Loss 2.391649\n",
            "[Iter 7600] Loss 2.214869\n",
            "[Iter 7700] Loss 1.970147\n",
            "[Iter 7800] Loss 2.251984\n",
            "[Iter 7900] Loss 2.204718\n",
            "[Iter 8000] Loss 2.384109\n",
            "[Iter 8100] Loss 1.999253\n",
            "[Iter 8200] Loss 2.090970\n",
            "[Iter 8300] Loss 1.923803\n",
            "[Iter 8400] Loss 2.606769\n",
            "[Iter 8500] Loss 1.888581\n",
            "[Iter 8600] Loss 1.907860\n",
            "[Iter 8700] Loss 2.006641\n",
            "[Iter 8800] Loss 2.030128\n",
            "[Iter 8900] Loss 1.604912\n",
            "[Iter 9000] Loss 1.740197\n",
            "[Iter 9100] Loss 1.858048\n",
            "[Iter 9200] Loss 2.015394\n",
            "[Iter 9300] Loss 1.748973\n",
            "[Iter 9400] Loss 1.940338\n",
            "[Iter 9500] Loss 1.886838\n",
            "[Iter 9600] Loss 2.154574\n",
            "[Iter 9700] Loss 1.934222\n",
            "[Iter 9800] Loss 1.968160\n",
            "[Iter 9900] Loss 1.584255\n",
            "[Iter 10000] Loss 1.613980\n",
            "[Iter 10100] Loss 1.874476\n",
            "[Iter 10200] Loss 1.808486\n",
            "[Iter 10300] Loss 1.666800\n",
            "[Iter 10400] Loss 1.944270\n",
            "[Iter 10500] Loss 1.565728\n",
            "[Iter 10600] Loss 1.548249\n",
            "[Iter 10700] Loss 1.629999\n",
            "[Iter 10800] Loss 1.761591\n",
            "[Iter 10900] Loss 1.706747\n",
            "[Iter 11000] Loss 1.692259\n",
            "[Iter 11100] Loss 1.677249\n",
            "[Iter 11200] Loss 1.838113\n",
            "[Iter 11300] Loss 1.759963\n",
            "[Iter 11400] Loss 1.872514\n",
            "[Iter 11500] Loss 1.682307\n",
            "[Iter 11600] Loss 1.740606\n",
            "[Iter 11700] Loss 1.623916\n",
            "[Iter 11800] Loss 1.938594\n",
            "[Iter 11900] Loss 1.842813\n",
            "[Iter 12000] Loss 1.688747\n",
            "[Iter 12100] Loss 1.466036\n",
            "[Iter 12200] Loss 1.525669\n",
            "[Iter 12300] Loss 1.712365\n",
            "[Iter 12400] Loss 1.715732\n",
            "[Iter 12500] Loss 1.673976\n",
            "[Iter 12600] Loss 1.522030\n",
            "[Iter 12700] Loss 1.583578\n",
            "[Iter 12800] Loss 1.599049\n",
            "[Iter 12900] Loss 1.717610\n",
            "[Iter 13000] Loss 1.695403\n",
            "[Iter 13100] Loss 1.350178\n",
            "[Iter 13200] Loss 1.550275\n",
            "[Iter 13300] Loss 1.592753\n",
            "[Iter 13400] Loss 1.316888\n",
            "[Iter 13500] Loss 1.277187\n",
            "[Iter 13600] Loss 1.362285\n",
            "[Iter 13700] Loss 1.400978\n",
            "[Iter 13800] Loss 1.336089\n",
            "[Iter 13900] Loss 1.455526\n",
            "[Iter 14000] Loss 1.354413\n",
            "[Iter 14100] Loss 1.392820\n",
            "[Iter 14200] Loss 1.424383\n",
            "[Iter 14300] Loss 1.394271\n",
            "[Iter 14400] Loss 1.286873\n",
            "[Iter 14500] Loss 1.366749\n",
            "[Iter 14600] Loss 1.376886\n",
            "[Iter 14700] Loss 1.435511\n",
            "[Iter 14800] Loss 1.348348\n",
            "[Iter 14900] Loss 1.349766\n",
            "[Iter 15000] Loss 1.384355\n",
            "[Iter 15100] Loss 1.272056\n",
            "[Iter 15200] Loss 1.288661\n",
            "[Iter 15300] Loss 1.158518\n",
            "[Iter 15400] Loss 1.171012\n",
            "[Iter 15500] Loss 1.269104\n",
            "[Iter 15600] Loss 1.231514\n",
            "[Iter 15700] Loss 1.216995\n",
            "[Iter 15800] Loss 1.070366\n",
            "[Iter 15900] Loss 1.151444\n",
            "[Iter 16000] Loss 1.072219\n",
            "[Iter 16100] Loss 1.256453\n",
            "[Iter 16200] Loss 1.265041\n",
            "[Iter 16300] Loss 1.282167\n",
            "[Iter 16400] Loss 1.316702\n",
            "[Iter 16500] Loss 1.136974\n",
            "[Iter 16600] Loss 1.024065\n",
            "[Iter 16700] Loss 1.197309\n",
            "[Iter 16800] Loss 1.145758\n",
            "[Iter 16900] Loss 1.426056\n",
            "[Iter 17000] Loss 1.227519\n",
            "[Iter 17100] Loss 0.959821\n",
            "[Iter 17200] Loss 1.291475\n",
            "[Iter 17300] Loss 0.998385\n",
            "[Iter 17400] Loss 1.114654\n",
            "[Iter 17500] Loss 1.009880\n",
            "[Iter 17600] Loss 1.212640\n",
            "[Iter 17700] Loss 0.999869\n",
            "[Iter 17800] Loss 1.191544\n",
            "[Iter 17900] Loss 1.081518\n",
            "[Iter 18000] Loss 1.172810\n",
            "[Iter 18100] Loss 0.898367\n",
            "[Iter 18200] Loss 0.985132\n",
            "[Iter 18300] Loss 0.963537\n",
            "[Iter 18400] Loss 0.966496\n",
            "[Iter 18500] Loss 0.987156\n",
            "[Iter 18600] Loss 1.088655\n",
            "[Iter 18700] Loss 0.929382\n",
            "[Iter 18800] Loss 1.023937\n",
            "[Iter 18900] Loss 1.010981\n",
            "[Iter 19000] Loss 1.014797\n",
            "[Iter 19100] Loss 0.816404\n",
            "[Iter 19200] Loss 0.838033\n",
            "[Iter 19300] Loss 0.957233\n",
            "[Iter 19400] Loss 0.723558\n",
            "[Iter 19500] Loss 0.924347\n",
            "[Iter 19600] Loss 0.861069\n",
            "[Iter 19700] Loss 0.838905\n",
            "[Iter 19800] Loss 0.938433\n",
            "[Iter 19900] Loss 1.003420\n",
            "[Iter 20000] Loss 1.092056\n",
            "[Iter 20100] Loss 0.849088\n",
            "[Iter 20200] Loss 0.852519\n",
            "[Iter 20300] Loss 1.015933\n",
            "[Iter 20400] Loss 0.975017\n",
            "[Iter 20500] Loss 0.766499\n",
            "[Iter 20600] Loss 1.118468\n",
            "[Iter 20700] Loss 0.922557\n",
            "[Iter 20800] Loss 0.945988\n",
            "[Iter 20900] Loss 0.917616\n",
            "[Iter 21000] Loss 0.913346\n",
            "[Iter 21100] Loss 0.844672\n",
            "[Iter 21200] Loss 0.762111\n",
            "[Iter 21300] Loss 0.881435\n",
            "[Iter 21400] Loss 0.987261\n",
            "[Iter 21500] Loss 0.787481\n",
            "[Iter 21600] Loss 0.967715\n",
            "[Iter 21700] Loss 0.934464\n",
            "[Iter 21800] Loss 0.919519\n",
            "[Iter 21900] Loss 0.734896\n",
            "[Iter 22000] Loss 0.878105\n",
            "[Iter 22100] Loss 0.761447\n",
            "[Iter 22200] Loss 0.783510\n",
            "[Iter 22300] Loss 0.888037\n",
            "[Iter 22400] Loss 0.769275\n",
            "[Iter 22500] Loss 0.839547\n",
            "[Iter 22600] Loss 0.708690\n",
            "[Iter 22700] Loss 0.795381\n",
            "[Iter 22800] Loss 0.870463\n",
            "[Iter 22900] Loss 0.708615\n",
            "[Iter 23000] Loss 0.650388\n",
            "[Iter 23100] Loss 0.818839\n",
            "[Iter 23200] Loss 0.613891\n",
            "[Iter 23300] Loss 0.766690\n",
            "[Iter 23400] Loss 0.909544\n",
            "[Iter 23500] Loss 0.720230\n",
            "[Iter 23600] Loss 0.628567\n",
            "[Iter 23700] Loss 0.922535\n",
            "[Iter 23800] Loss 0.628185\n",
            "[Iter 23900] Loss 0.660225\n",
            "[Iter 24000] Loss 0.804785\n",
            "[Iter 24100] Loss 0.749068\n",
            "[Iter 24200] Loss 0.690205\n",
            "[Iter 24300] Loss 0.753294\n",
            "[Iter 24400] Loss 0.697032\n",
            "[Iter 24500] Loss 0.696432\n",
            "[Iter 24600] Loss 0.640654\n",
            "[Iter 24700] Loss 0.648635\n",
            "[Iter 24800] Loss 0.772710\n",
            "[Iter 24900] Loss 0.748536\n",
            "[Iter 25000] Loss 0.769747\n",
            "[Iter 25100] Loss 0.702951\n",
            "[Iter 25200] Loss 0.720359\n",
            "[Iter 25300] Loss 0.610238\n",
            "[Iter 25400] Loss 0.538352\n",
            "[Iter 25500] Loss 0.534352\n",
            "[Iter 25600] Loss 0.643484\n",
            "[Iter 25700] Loss 0.577422\n",
            "[Iter 25800] Loss 0.504892\n",
            "[Iter 25900] Loss 0.655091\n",
            "[Iter 26000] Loss 0.559628\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWc39D8FJ2bfvoj9VAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdoPMiLlyN3R",
        "colab_type": "text"
      },
      "source": [
        "### Part (d) -- 2 pt\n",
        "\n",
        "This model requires many epochs (>50) to train, and is quite slow without using a GPU.\n",
        "You can train a model yourself, or you can load the model weights that we have trained,\n",
        "and available on the course website https://www.cs.toronto.edu/~lczhang/321/files/p4model.pk (11MB).\n",
        "\n",
        "Assuming that your `AutoEncoder` is set up correctly, the following code should run without\n",
        "error."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWf8EDagyN3S",
        "colab_type": "code",
        "outputId": "76fa992e-fce6-451c-e41b-4b2223967358",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "model = AutoEncoder(10000, 128, 128)\n",
        "checkpoint_path = '/content/gdrive/My Drive/CSC321/p4model.pk' # Update me\n",
        "model.load_state_dict(torch.load(checkpoint_path))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 147
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5qrcxARyN3T",
        "colab_type": "text"
      },
      "source": [
        "Then, repeat your code from Q2(d), for `train_data[10].title`\n",
        "with temperature settings 0.7, 0.9, and 1.5.\n",
        "Explain why we generally don't want the temperature setting to\n",
        "be too **small**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zEe2JyonyN3U",
        "colab_type": "code",
        "outputId": "83f1d64f-1a8e-4673-acc8-52d04e659691",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        }
      },
      "source": [
        "# Include the generated sequences and explanation in your PDF report.\n",
        "\n",
        "headline = train_data[10].title\n",
        "print(headline)\n",
        "input_seq = torch.Tensor([vocab.stoi[w] for w in headline]).unsqueeze(0).long()\n",
        "\n",
        "print(\"sequences generated with temperature of 0.5:\\n\")\n",
        "for it in range(5):\n",
        "  hidden = model.encode(input_seq)\n",
        "  print(sample_sequence(model, hidden, temperature=0.5))\n",
        "\n",
        "print(\"sequences generated with temperature of 0.7:\\n\")\n",
        "for it in range(5):\n",
        "  hidden = model.encode(input_seq)\n",
        "  print(sample_sequence(model, hidden, temperature=0.7))\n",
        "print(\"\\n\\nsequences generated with temperature of 0.9:\\n\")\n",
        "for it in range(5):\n",
        "  hidden = model.encode(input_seq)\n",
        "  print(sample_sequence(model, hidden, temperature=0.9))\n",
        "\n",
        "print(\"sequences generated with temperature of 1:\\n\")\n",
        "for it in range(5):\n",
        "  hidden = model.encode(input_seq)\n",
        "  print(sample_sequence(model, hidden, temperature=1))\n",
        "\n",
        "print(\"\\n\\nsequences generated with temperature of 1.5:\\n\")\n",
        "for it in range(5):\n",
        "  hidden = model.encode(input_seq)\n",
        "  print(sample_sequence(model, hidden, temperature=1.5))\n",
        "#small temperture causes the generated sequence to be drawn towards a specific set of words "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['<bos>', 'wall', 'street', 'rises', ',', 'limps', 'across', 'the', 'finish', 'line', 'of', 'a', 'turbulent', 'year', '<eos>']\n",
            "sequences generated with temperature of 0.5:\n",
            "\n",
            "['wall', 'street', 'rises', ',', 'limps', 'die', 'win', 'at', '$', '<pad>', 'highway', 'a', 'initial']\n",
            "['wall', 'street', 'rises', ',', 'limps', 'die', 'win', 'at', '$', '<pad>', 'highway', 'election', 'worries']\n",
            "['wall', 'street', 'rises', ',', 'limps', 'die', 'win', 'at', 'of', 'sciences', '<pad>', 'presidential', 'heads']\n",
            "['wall', 'street', 'rises', ',', 'limps', 'die', 'win', 'at', 'of', 'sciences', '<pad>', 'presidential', 'and']\n",
            "['wall', 'street', 'rises', ',', 'limps', 'die', 'win', 'at', 'of', 'sciences', 'election', 'four']\n",
            "sequences generated with temperature of 0.7:\n",
            "\n",
            "['wall', 'street', 'rises', ',', 'limps', 'die', 'win', \"'s\", 'pulls', 'peace', '<pad>', 'after', 'another']\n",
            "['wall', 'street', 'rises', ',', 'limps', 'across', 'the', 'finish', ',', 'line', 'next', 'year', 'fire']\n",
            "['wall', 'street', 'rises', ',', 'limps', 'die', 'win', 'at', '$', 'election', ';', 'pay', 'activism']\n",
            "['wall', 'street', 'rises', ',', 'limps', 'die', 'win', 'at', '$', '<pad>', 'highway', 'a', 'bets']\n",
            "['wall', 'street', 'rises', ',', 'limps', 'across', 'melbourne', 'france', 'an', 'percent', '_num_', 'brexit', 'disruption']\n",
            "\n",
            "\n",
            "sequences generated with temperature of 0.9:\n",
            "\n",
            "['wall', 'street', 'rises', ',', 'limps', 'dialogue', 'win', 'of', 'than', '<pad>', 'in', 'frontier', 'safe-haven']\n",
            "['wall', 'street', 'rises', ',', 'limps', 'across', 'the', 'finish', 'line', ';', 'of', 'election', 'surges']\n",
            "['wall', 'street', 'rises', ',', 'limps', 'die', 'win', \"'s\", 'spurs', 'boom', '<pad>', 'mulls', 'protest']\n",
            "['wall', 'street', 'rises', ',', 'limps', 'open', 'sentence', ',', 'second', 'graphic', 'as', 'big', 'deliveroo']\n",
            "['wall', 'street', 'rises', ',', 'limps', 'die', 'win', 'at', '$', '<pad>', 'highway', 'high', 'woes']\n",
            "sequences generated with temperature of 1:\n",
            "\n",
            "['wall', 'street', 'rises', ',', 'limps', 'open', 'websites', ',', 'block', 'kenya', 'to', 'one-time', 'local']\n",
            "['wall', 'street', 'rises', ',', 'limps', 'open', 'sentence', ',', 'jimmy', 'china', 'a', 'big', 'sensor']\n",
            "['wall', 'street', 'rises', ',', 'hut', 'kyi', 'one', 'withdraws', 'update', 'hit', 'after', 'petrol', 'of']\n",
            "['wall', 'street', 'rises', ',', 'basis', 'philippine', 'australia', 'families', 'profit', ',', '<pad>', 'voluntary', '-', 'out']\n",
            "['wall', 'street', 'rises', ',', 'limps', 'across', 'the', 'finish', 'line', 'of', 'a', 'turbulent', 'year']\n",
            "\n",
            "\n",
            "sequences generated with temperature of 1.5:\n",
            "\n",
            "['wall', 'street', 'rises', ',', 'pesos', '3,000', 'indians', 'open', 'to', 'brooklyn', 'five', 'picks', 'year']\n",
            "['wall', 'street', 'rises', ',', 'tsitsipas', 'pemex', 'coors', 'race', 'an', 'in', '<pad>', 'january', 'gain']\n",
            "['wall', 'street', 'rises', ',', 'hut', 'faces', 'shoots', 'tackle', '<unk>', '<pad>', 'from', 'hangs', 'libya']\n",
            "['wall', 'street', 'rises', ',', 'limps', 'dialogue', 'marathon', ',', 'for', '<pad>', 'brakes', 'a', 'another']\n",
            "['wall', 'street', 'rises', ',', 'limps', 'refugee', 'upsets', 'continues', 'to', 'strikes', '<pad>', 'site', 'latest']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoativLnazmN",
        "colab_type": "text"
      },
      "source": [
        "**explaination**: small temperture causes the generated sequence to be drawn towards a specific set of words and decreases variety in the generated sequence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4MHXUzNyN3W",
        "colab_type": "text"
      },
      "source": [
        "## Question 4\n",
        "\n",
        "In parts 2-3, we've explored the decoder portion of the autoencoder. In this section,\n",
        "let's explore the **encoder**. In particular, the encoder RNN gives us \n",
        "embeddings of news headlines!\n",
        "\n",
        "First, let's load the **validation** data set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGj4jYpVyN3W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "valid_data = torchtext.data.TabularDataset(\n",
        "    path=valid_path,                # data file path\n",
        "    format=\"tsv\",                   # fields are separated by a tab\n",
        "    fields=[('title', text_field)]) # list of fields (we have only one)\n",
        "\n",
        "valid_iter = torchtext.data.BucketIterator(valid_data,\n",
        "                                          batch_size=128,\n",
        "                                          sort_key=lambda x: len(x.title), # to minimize padding\n",
        "                                          repeat=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itqRGN3vyN3Z",
        "colab_type": "text"
      },
      "source": [
        "### Part (a) -- 2 pt\n",
        "\n",
        "Compute the embeddings of every item in the validation set. Then, store the\n",
        "result in a single PyTorch tensor of shape `[19046, 128]`, since there are\n",
        "19,046 headlines in the validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sGsxSacyN3a",
        "colab_type": "code",
        "outputId": "b2a7d073-5796-4401-80fb-85e75fbd7ff6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "val_out = torch.Tensor([])\n",
        "for it, ((xs, lengths), _) in enumerate(valid_iter):\n",
        "  output= model.encode(xs).view(-1,128)\n",
        "  val_out = torch.cat((val_out, output))\n",
        "\n",
        "print(val_out.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([19046, 128])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tclHtBmgyN3c",
        "colab_type": "text"
      },
      "source": [
        "### Part (b) -- 2 pt\n",
        "\n",
        "Find the 5 closest headlines to the headline `valid_data[13]`. Use the\n",
        "cosine similarity to determine closeness. (Hint: You can use code from Project 2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9Z3K1OoyN3c",
        "colab_type": "code",
        "outputId": "c0aed2a6-f9fd-48de-dc4b-130893ae1f50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "# Write your code here. Make sure to include the actual 5 closest headlines.\n",
        "np_val_out = val_out.detach().numpy()\n",
        "norms = np.linalg.norm(np_val_out, axis=1)\n",
        "word_emb_norm = (np_val_out.T / norms).T\n",
        "similarities = np.matmul(word_emb_norm, word_emb_norm.T)\n",
        "\n",
        "def compute_k_closest(i, k):\n",
        "   closests = sorted(enumerate(similarities[:, i]),\n",
        "                      key=lambda r: r[1],\n",
        "                      reverse=True)\n",
        "   closest_indices = [index for (index, dist) in closests[:k]]\n",
        "   headlines = []\n",
        "   for j in closest_indices:\n",
        "     headlines.append(valid_data[j].title)\n",
        "   return headlines\n",
        "\n",
        "k_closests = []\n",
        "k = 5\n",
        "\n",
        "k_closests = compute_k_closest(13, k)\n",
        "\n",
        "print(\"closest headlines to headline:  \"+\" \" .join(valid_data[13].title)+ \" are: \")\n",
        "for headline in k_closests:\n",
        "  print(' '.join(headline))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "closest headlines to headline:  <bos> asia takes heart from new year gains in u.s. stock futures <eos> are: \n",
            "<bos> asia takes heart from new year gains in u.s. stock futures <eos>\n",
            "<bos> coronavirus travel slowdown spreads from china across asia-study <eos>\n",
            "<bos> as china creeps in , hong kongers want out <eos>\n",
            "<bos> u.s. softens position on israeli settlements <eos>\n",
            "<bos> botswana 's masisi retains presidency as bdp wins election <eos>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qcado2llyN3g",
        "colab_type": "text"
      },
      "source": [
        "### Part (c) -- 2 pt\n",
        "\n",
        "Find the 5 closest headlines to another headline of your choice."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5YngSSlyN3g",
        "colab_type": "code",
        "outputId": "ef95bd1d-a087-4bcd-d45e-194b7a347565",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        }
      },
      "source": [
        "# Write your code here. \n",
        "# Make sure to include the original headline and the 5 closest headlines.\n",
        "h = 1000\n",
        "k_closests = []\n",
        "k = 5\n",
        "k_closest = compute_k_closest(h, k)\n",
        "print(\"closest headlines to headline  \"+' '.join(valid_data[h].title) + \" are: \")\n",
        "for headline in k_closest:\n",
        "  print(' '.join(headline))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "closest headlines to headline  <bos> fintech companies raised a record $ _num_ bln in _num_ -research <eos> are: \n",
            "<bos> fintech companies raised a record $ _num_ bln in _num_ -research <eos>\n",
            "<bos> palestinians urge egypt , jordan to reconsider going to u.s.-led conference <eos>\n",
            "<bos> india air strike in pakistan territory killed _num_ militants : government source <eos>\n",
            "<bos> timeline : key events in huawei cfo meng wanzhou 's extradition case <eos>\n",
            "<bos> climate activists disrupt london concrete plant <eos>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4wHMxvayN3i",
        "colab_type": "text"
      },
      "source": [
        "### Part (d) -- 4 pts\n",
        "\n",
        "Choose two headlines from the validation set, and find their embeddings.\n",
        "We will **interpolate** between the two embeddings like we did in\n",
        "https://www.cs.toronto.edu/~lczhang/321/lec/autoencoder_notes.html\n",
        "\n",
        "Find 3 points, equally spaced between the embeddings of your headlines.\n",
        "If we let $e_0$ be the embedding of your first headline and $e_4$ be\n",
        "the embedding of your second headline, your three points should be:\n",
        "\n",
        "\\begin{align*}\n",
        "e_1 &=  0.75 e_0 + 0.25 e_4 \\\\\n",
        "e_2 &=  0.50 e_0 + 0.50 e_4 \\\\\n",
        "e_3 &=  0.25 e_0 + 0.75 e_4 \\\\\n",
        "\\end{align*}\n",
        "\n",
        "Decode each of $e_1$, $e_2$ and $e_3$ five times, with a temperature setting\n",
        "that shows some variation in the generated sequences, while generating sequences\n",
        "that makes sense."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDSgB0ZuyN3i",
        "colab_type": "code",
        "outputId": "3eb5f425-4319-4904-abdb-c810de592a23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        }
      },
      "source": [
        "e0 = val_out[100].view(1,1,128)\n",
        "e4 = val_out[50].view(1,1,128)\n",
        "e1 = 0.75*e0 + 0.25*e4\n",
        "e2 = 0.50*e0 + 0.50*e4 \n",
        "e3 = 0.25*e0 + 0.75*e4\n",
        "for it in range(5):\n",
        "  print(sample_sequence(model, e1, temperature=0.85))\n",
        "for it in range(5):\n",
        "  print(sample_sequence(model, e2, temperature=0.85))\n",
        "for it in range(5):\n",
        "  print(sample_sequence(model, e3, temperature=0.85))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['press', 'world', 'sue', 'business', 'december', 'order', 'debut', 'first-quarter']\n",
            "['press', 'world', 'probe', 'rules', 'tops', 'thyssenkrupp', 'bln', 'reach']\n",
            "['press', 'plans', 'cross', 'growth', 'fraud', 'uranium', 'button', 'group']\n",
            "['press', 'plans', 'treasury', 'rise', 'boss', '-', 'oct', 'at']\n",
            "['press', 'world', 'secures', 'of', 'british', 'belgium', 'pct', 'sets']\n",
            "['press', '<unk>', 'traders', 'in', 'chief', 'car', 'for', 'repo']\n",
            "['press', '<unk>', 'traders', 'in', 'official', 'flow', 'at', 'estate']\n",
            "['official', 'disease', 'poland', 'in', 'official', 'licences', 'management', \"'s\"]\n",
            "['press', '<unk>', 'german', 'activity', 'his', '_num_', 'huawei', 'record', 'process']\n",
            "['press', '<unk>', 'rebukes', 'are', \"'s\", 'deficit', 'listing', 'risk', 'addition']\n",
            "['china', 'state', 'media', 'submit', 'in', 'firm', 'for', 'open', 'results']\n",
            "['china', 'state', 'media', 'submit', 'in', 'firm', 'for', 'open', 'forex-dollar']\n",
            "['china', 'state', 'calls', 'americas', 'new', 'to', 'alzheimer', '_num_', 'bets']\n",
            "['china', 'state', 'media', 'submit', 'in', 'firm', 'for', 'open', 'slip', 'until']\n",
            "['china', 'state', 'media', 'submit', 'in', 'firm', 'for', 'open', 'doubts']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}